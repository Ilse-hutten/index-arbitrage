{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA for PF Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (0.2.54)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from yfinance) (4.3.6)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: pytz>=2022.5 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from yfinance) (2025.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from yfinance) (2.1.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from yfinance) (4.13.3)\n",
      "Requirement already satisfied: requests>=2.31 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from yfinance) (3.17.9)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from requests>=2.31->yfinance) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from requests>=2.31->yfinance) (3.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/sb/.pyenv/versions/3.10.6/envs/stockify_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VkUCrCsHaWJn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 13:03:11.612827: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-19 13:03:11.624837: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-19 13:03:11.721979: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-19 13:03:11.819865: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742389391.893329  681639 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742389391.915745  681639 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742389392.096307  681639 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742389392.096360  681639 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742389392.096366  681639 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742389392.096370  681639 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-19 13:03:12.118199: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../app\"))\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = '/home/sb/lewagon_london/project_lewagon/stock-stat-replica/data/lewagon-statistical-arbitrage-ae470f7dcd48.json'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_querry'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdata_querry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m  fetch_ftse100_all_components,fetch_ftse100_index\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m fetch_ftse100_all_components()\n\u001b[1;32m      3\u001b[0m index_data\u001b[38;5;241m=\u001b[39mfetch_ftse100_index()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_querry'"
     ]
    }
   ],
   "source": [
    "from data_querry import  fetch_ftse100_all_components,fetch_ftse100_index\n",
    "data = fetch_ftse100_all_components()\n",
    "index_data=fetch_ftse100_index()\n",
    "data=data.merge(index_data,on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('date', inplace=True)\n",
    "\n",
    "df.fillna(method='ffill', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WySLTUq6yPvE"
   },
   "outputs": [],
   "source": [
    "# Create log returns to remove stationarity\n",
    "log_returns = np.log(df / df.shift(1))\n",
    "\n",
    "# Drop NaN values caused by the shift\n",
    "log_returns = log_returns.dropna().drop(columns=\"FTSE100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cH2XPTCOpxn0",
    "outputId": "6e5592dd-e26c-4090-fef6-db5a93a03175"
   },
   "outputs": [],
   "source": [
    "# Creating X of the closing prices (absolute prices)\n",
    "X_abs = df.drop(columns=\"FTSE100\").dropna()\n",
    "stock_features = X_abs.columns\n",
    "X_abs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JlJm6by4Dls",
    "outputId": "e02b90b6-8f7d-4753-c96d-7c67819e9634"
   },
   "outputs": [],
   "source": [
    "# Creating X of the closing prices (log returns)\n",
    "X_log = log_returns.copy()\n",
    "stock_log_features = X_log.columns\n",
    "X_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "fHbNcktBZ-aD",
    "outputId": "80175906-1139-41ed-8fb5-1dfe1dfa3e44"
   },
   "outputs": [],
   "source": [
    "# Preprocessing ABS (data must be centered around its mean before PCA)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_abs)\n",
    "X_abs = pd.DataFrame(scaler.transform(X_abs), columns=stock_features)\n",
    "X_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "8Mz4vyRo4ph7",
    "outputId": "5f62a86d-90a5-4b26-e1f0-126004917303"
   },
   "outputs": [],
   "source": [
    "# Preprocessing LOG (data must be centered around its mean before PCA)\n",
    "scaler.fit(X_log)\n",
    "X_log = pd.DataFrame(scaler.transform(X_log), columns=stock_log_features, index=log_returns.index)\n",
    "X_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "-9Dxfx9cnK1C",
    "outputId": "15bd0a5c-6029-4200-a104-0e3721354367"
   },
   "outputs": [],
   "source": [
    "# Compute Principal Components\n",
    "pca = PCA()\n",
    "pca.fit(X_abs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVJDXMfdsVbm"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "Ghr5TbEWsWIi",
    "outputId": "c39d1574-0906-44ed-a12a-efa5e78bfa5d"
   },
   "outputs": [],
   "source": [
    "# Access PCs\n",
    "W = pca.components_\n",
    "# Print PCs as COLUMNS\n",
    "T = pd.DataFrame(W.T,\n",
    "                 index=stock_features,\n",
    "                 columns=[f'PC{i}' for i in range(1, pca.n_components_+1)])\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "n2n4ohILFc0r",
    "outputId": "9b88390c-501a-4c24-c7ce-332eb2496ed0"
   },
   "outputs": [],
   "source": [
    "# Compute Principal Components log returns\n",
    "pca2 = PCA()\n",
    "pca2.fit(X_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "1gzDAC0R5N0c",
    "outputId": "34e2862a-d985-479b-a182-8467ac4367a2"
   },
   "outputs": [],
   "source": [
    "# Access PCs\n",
    "W2 = pca2.components_\n",
    "# Print PCs as COLUMNS\n",
    "T_log = pd.DataFrame(W2.T,\n",
    "                 index=stock_log_features,\n",
    "                 columns=[f'PC{i}' for i in range(1, pca2.n_components_+1)])\n",
    "T_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "LmrgaCWf7CLz",
    "outputId": "36db1b87-f358-4b5a-d362-2ebe26702122"
   },
   "outputs": [],
   "source": [
    "# pca.explained_variance_ratio_ and pca2.explained_variance_ratio_ (absolute vs log return)\n",
    "explained_variance_1 = pca.explained_variance_ratio_\n",
    "explained_variance_2 = pca2.explained_variance_ratio_\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the first PCA - abs\n",
    "axes[0].plot(explained_variance_1)\n",
    "axes[0].set_title('PCA 1 - Explained Variance using abs prices')\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('% Explained Variance')\n",
    "\n",
    "# Plot the second PCA - log\n",
    "axes[1].plot(explained_variance_2)\n",
    "axes[1].set_title('PCA 2 - Explained Variance using log return')\n",
    "axes[1].set_xlabel('Principal Component')\n",
    "axes[1].set_ylabel('% Explained Variance')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "lh7XG3RY8a2P",
    "outputId": "3dce199a-58f7-4cf1-fec2-548fc6ac0829"
   },
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))  # 1 row, 2 columns\n",
    "\n",
    "# first PCA - ABS\n",
    "axes[0].plot(np.cumsum(explained_variance_1))\n",
    "axes[0].set_title('cumulated share of explained variance using stock prices')\n",
    "axes[0].set_xlabel('# of principal component used')\n",
    "\n",
    "# second PCA - log\n",
    "axes[1].plot(np.cumsum(explained_variance_2))\n",
    "axes[1].set_title('cumulated share of explained variance using log return')\n",
    "axes[1].set_xlabel('# of principal component used')\n",
    "\n",
    "# Adjust the y-axis scale\n",
    "axes[0].set_ylim(0.4, 1.0)  # Adjust scale for subplot 1\n",
    "axes[1].set_ylim(0.2, 1.0)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "ZGdMiZYzx4Xd",
    "outputId": "d8d355ac-2890-4522-ca2e-f5aa90755770"
   },
   "outputs": [],
   "source": [
    "# Picking stocks most closely mimicking the index (based on log returns method)\n",
    "\n",
    "# Keeping only PC1 and PC2\n",
    "T_log = T_log.iloc[:, :4]\n",
    "\n",
    "# adding column showing cum PC1+PC2\n",
    "T_log[\"PC_sum\"] = T_log[\"PC1\"]+T_log[\"PC2\"]+T_log[\"PC3\"]+T_log[\"PC4\"]\n",
    "\n",
    "# Sort by PC_sum\n",
    "T_log_sorted = T_log.sort_values(\"PC_sum\", ascending=False)\n",
    "T_log_sorted = T_log_sorted.reset_index()\n",
    "T_log_sorted.rename(columns={T_log_sorted.columns[0]: \"Stocks\" }, inplace = True)\n",
    "\n",
    "# Calculate the sum of 'PC_sum' for the top 5 rows\n",
    "top_5_sum = T_log_sorted[\"PC_sum\"].head(5).sum()\n",
    "\n",
    "# Add the 'pf_weights' column by dividing each stock's 'PC_sum' by the top 5 sum\n",
    "T_log_sorted[\"pf_weights\"] = T_log_sorted[\"PC_sum\"] /top_5_sum\n",
    "# Set weights to 0 for rows beyond the top 5\n",
    "T_log_sorted.loc[5:, \"pf_weights\"] = 0\n",
    "\n",
    "T_log_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "OAv9aC4p2_j4",
    "outputId": "58ff1c2f-0171-4368-ff8b-c9b3bc92d963"
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "# Index the data by dividing each stock's prices by its first value\n",
    "indexed_data = df / df.iloc[0] * 100\n",
    "\n",
    "# Retrieve the top 5 stocks from the \"Stocks\" column of T_log_sorted\n",
    "top_5_stocks = T_log_sorted[\"Stocks\"].head(5)\n",
    "\n",
    "# Plot the FTSE index\n",
    "plt.plot(indexed_data[\"FTSE100\"], label=\"FTSE100\", linewidth=2)\n",
    "\n",
    "# Plot each of the top 5 stocks dynamically\n",
    "for stock in top_5_stocks:\n",
    "    if stock in indexed_data.columns:  # Ensure the stock is in the data\n",
    "        plt.plot(indexed_data[stock], label=stock)\n",
    "\n",
    "# Add a legend to distinguish the lines\n",
    "plt.legend()\n",
    "\n",
    "# Format the dates on the x-axis\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=6))  # Show dates every 1 month\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))  # Format as \"Month Year\"\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title(\"Indexed Closing Prices of Top 5 Stocks and FTSE\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Indexed Price (Base = 100)\")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "id": "Jej6QRlE9Npp",
    "outputId": "2b075b0e-7599-49c3-c845-125364b8f487"
   },
   "outputs": [],
   "source": [
    "# Extract the top 5 stocks and their weights to make a replicating portfolio\n",
    "top_5_stocks = T_log_sorted[\"Stocks\"].head(5)\n",
    "top_5_weights = T_log_sorted[\"pf_weights\"].head(5)\n",
    "\n",
    "# Calculate the weighted sumproduct (dot product of weights and indexed returns)\n",
    "weighted_returns = (indexed_data[top_5_stocks] * top_5_weights.values).sum(axis=1)\n",
    "\n",
    "# Plot the weighted returns\n",
    "plt.plot(weighted_returns, label=\"Synthetic Portfolio\", color=\"blue\", linewidth=2)\n",
    "\n",
    "# Plot the FTSE index for comparison\n",
    "plt.plot(indexed_data[\"FTSE100\"], label=\"FTSE\", color=\"orange\", linestyle=\"--\")\n",
    "\n",
    "# Add legend, labels, and title\n",
    "plt.legend()\n",
    "plt.title(\"Synthetic Portfolio vs FTSE Index\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Weighted Indexed Returns\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjMGPMBZ3ABP"
   },
   "outputs": [],
   "source": [
    "window = 100 # n of trading days\n",
    "dates = X_log.index[window:]  # Align dates with the end of each rolling window\n",
    "\n",
    "# Placeholder for summed PCs for all stocks (full data)\n",
    "summed_pcs_full = {}\n",
    "\n",
    "def rolling_pca(window_start):\n",
    "    pca_roll = PCA()\n",
    "    # Create the rolling window data excluding today's data\n",
    "    window_data = X_log.iloc[window_start:window_start + window - 1, :]  # Exclude today's row\n",
    "    # Fit PCA to the rolling window\n",
    "    pca_roll.fit(window_data)\n",
    "    # Extract loadings (components matrix)\n",
    "    loadings_matrix = pca_roll.components_.T  # Transpose to get stocks as rows\n",
    "\n",
    "    # Sum the first 4 PCs for each stock\n",
    "    summed_values = loadings_matrix[:, :4].sum(axis=1)  # Sum across the first 4 PCs\n",
    "    summed_pcs_full[dates[window_start]] = pd.Series(summed_values, index=X_log.columns)  # Store as Series\n",
    "\n",
    "# Iterate through rolling windows\n",
    "for start in range(len(X_log) - window):\n",
    "    rolling_pca(start)\n",
    "\n",
    "# Combine results into a full DataFrame (dates as rows, stocks as columns)\n",
    "summed_pcs_full_df = pd.DataFrame(summed_pcs_full).T  # Transpose to align dates as rows\n",
    "summed_pcs_full_df.index.name = \"Date\"  # Set index name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YR2ZUdIt4XHk",
    "outputId": "fec7d9df-9bf5-46c3-bd11-c412c63609d9"
   },
   "outputs": [],
   "source": [
    "# Placeholder for daily portfolio weights\n",
    "daily_portfolio = []\n",
    "\n",
    "# Loop through each date\n",
    "for date in summed_pcs_full_df.index:\n",
    "    # Get summed PCs for all stocks on this date\n",
    "    daily_values = summed_pcs_full_df.loc[date]\n",
    "\n",
    "    # Select the top 10 stocks for this date\n",
    "    top10_stocks = daily_values.nlargest(10)  # Top 10 stocks by summed PCs\n",
    "\n",
    "    # Normalize the summed PCs to use as portfolio weights\n",
    "    portfolio_weights = top10_stocks / top10_stocks.sum()  # calc weights\n",
    "\n",
    "    # Store the portfolio details (date, stocks, weights)\n",
    "    portfolio_details = {\n",
    "        \"Date\": date,\n",
    "        \"Stocks\": list(top10_stocks.index),\n",
    "        \"Weights\": list(portfolio_weights.values)\n",
    "    }\n",
    "    daily_portfolio.append(portfolio_details)\n",
    "\n",
    "# Convert to a structured DataFrame\n",
    "daily_portfolio_df = pd.DataFrame(daily_portfolio)\n",
    "\n",
    "print(\"Daily Portfolio with Stocks and Weights:\")\n",
    "print(daily_portfolio_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "7n7sTRno42As",
    "outputId": "ed00db08-ac1f-4048-c1f6-3f3083d5e1d2"
   },
   "outputs": [],
   "source": [
    "daily_portfolio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caLsngPbBxn7",
    "outputId": "a58af09f-9cc7-4204-9a76-0dcab4ef9d9c"
   },
   "outputs": [],
   "source": [
    "# Placeholder for daily weights across all stocks\n",
    "daily_weights = []\n",
    "\n",
    "# Loop through each date in the summed PCs DataFrame\n",
    "for date in summed_pcs_full_df.index:\n",
    "    # Get summed PCs for all stocks on this date\n",
    "    daily_values = summed_pcs_full_df.loc[date]\n",
    "\n",
    "    # Select the top 10 stocks for this date\n",
    "    top10_stocks = daily_values.nlargest(10)  # Top 10 stocks by summed PCs\n",
    "\n",
    "    # calc stock weights\n",
    "    portfolio_weights = top10_stocks / top10_stocks.sum()\n",
    "\n",
    "    # Create a row of weights with 0 for stocks not in the top 10\n",
    "    row_weights = pd.Series(0.0, index=summed_pcs_full_df.columns)  # Initialize with zeros\n",
    "    row_weights[top10_stocks.index] = portfolio_weights\n",
    "\n",
    "    # Add the row of weights to the daily weights list\n",
    "    daily_weights.append(row_weights)\n",
    "\n",
    "# Create df\n",
    "daily_weights_df = pd.DataFrame(daily_weights, index=summed_pcs_full_df.index)\n",
    "\n",
    "print(\"Daily Weights Matrix (All Stocks, Top 10 Weighted):\")\n",
    "print(daily_weights_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Am-JSXb-B1mw",
    "outputId": "7b4a81ec-aac4-41ac-c40f-dd7382f554d3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daily_weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_date = '2025-03-07'\n",
    "\n",
    "# Filter stocks with values > 0 on the specific date\n",
    "filtered_columns = daily_weights_df.loc[pca_date][daily_weights_df.loc[pca_date] > 0].index\n",
    "rep_pf = daily_weights_df.loc[[pca_date], filtered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rep_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_pf_log_returns_daily = log_returns[rep_pf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_pf_log_returns_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to show replication backwards, not backtest\n",
    "rep_pf_results = rep_pf_log_returns_daily.loc[:pca_date].mul(rep_pf.iloc[0], axis=1)\n",
    "rep_pf_results[\"total_rep_pf\"] = rep_pf_results.sum(axis=1)\n",
    "rep_pf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log returns FTSE100\n",
    "FTSE_log_return = np.log(df[\"FTSE100\"] / df[\"FTSE100\"].shift(1)).loc[:pca_date]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming FTSE_return and rep_pf_results[\"total_rep_pf\"] are pandas Series with datetime indices\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot FTSE returns\n",
    "plt.plot(FTSE_log_return, label=\"FTSE 100 Returns\", color=\"blue\", linewidth=2)\n",
    "\n",
    "# Plot Synthetic Portfolio returns\n",
    "plt.plot(rep_pf_results[\"total_rep_pf\"], label=\"Synthetic Portfolio Returns\", color=\"orange\", linewidth=2)\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"FTSE 100 vs Synthetic Portfolio Returns\", fontsize=16)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Cumulative Returns\", fontsize=12)\n",
    "\n",
    "# Format x-axis for dates\n",
    "plt.xticks(rotation=45, fontsize=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert daily returns to cumulative returns starting at 100\n",
    "FTSE_cumulative = (FTSE_log_return + 1).cumprod() * 100  # Convert to cumulative returns\n",
    "rep_pf_cumulative = (rep_pf_results[\"total_rep_pf\"] + 1).cumprod() * 100  # Convert to cumulative returns\n",
    "\n",
    "# Plot the cumulative returns\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot FTSE cumulative returns\n",
    "plt.plot(FTSE_cumulative, label=\"FTSE 100 (Starting at 100)\", color=\"blue\", linewidth=2)\n",
    "\n",
    "# Plot Synthetic Portfolio cumulative returns\n",
    "plt.plot(rep_pf_cumulative, label=\"Synthetic Portfolio (Starting at 100)\", color=\"orange\", linewidth=2)\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"FTSE 100 vs Synthetic Portfolio (Cumulative Returns)\", fontsize=16)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Cumulative Returns (Starting at 100)\", fontsize=12)\n",
    "\n",
    "# Format x-axis for dates\n",
    "plt.xticks(rotation=45, fontsize=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTSE_cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_pf_cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spread_df = pd.merge(rep_pf_cumulative, FTSE_cumulative, on='date', how='inner') \n",
    "spread_df[\"spread\"] = spread_df[\"FTSE100\"] - spread_df[\"total_rep_pf\"]\n",
    "spread_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#long if z score = less - 0.5     but not smaller than - 2   ;   short if over 0.5 but less then 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtesting : calculate z score on rolling basis of past 60 days (t 1 is the pca input, and then calc returns post t1 based on z score strategy)    -1: short rep, long ftse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Spread with LSTM / NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_economic_indicators(start_date, end_date):\n",
    "#     tickers = [\n",
    "#         \"^GSPC\",   # S&P 500\n",
    "#         \"^VIX\",    # Volatility Index\n",
    "#         \"^TNX\",    # 10-Year Treasury Yield\n",
    "#         \"^FVX\",    # 5-Year Treasury Yield\n",
    "#         \"GC=F\",    # Gold Futures\n",
    "#         \"CL=F\",    # Crude Oil Futures\n",
    "#         \"GBPUSD=X\" # GBP/USD Exchange Rate\n",
    "#     ]\n",
    "    \n",
    "#     data = yf.download(tickers, start=start_date, end=end_date)\n",
    "#     data = data['Close']    \n",
    "#     return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# economic_indicators = get_economic_indicators('2022-01-31', '2025-03-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstickers = {\n",
    "#         \"^GSPC\":\"SP500\",   \n",
    "#         \"^VIX\":\"VIX\",    \n",
    "#         \"^TNX\":\"TNX\",    \n",
    "#         \"^FVX\":\"FVX\",      \n",
    "#         \"GC=F\":\"GF\",    \n",
    "#         \"CL=F\":\"COF\",   \n",
    "#         \"GBPUSD=X\":\"GBPUSD\" \n",
    "# }\n",
    "# eco_df = economic_indicators.rename(columns=abstickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_querry  import eco_df\n",
    "bg_data=eco_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco_df=bg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco_df.fillna(method='ffill', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def convert_to_log_returns(data):\n",
    "    log_returns = data.copy()\n",
    "    \n",
    "    # List of indicators to convert to log returns\n",
    "    convert_tickers = ['SP500', 'GF', 'COF', 'GBPUSD']\n",
    "    \n",
    "    # Apply log returns conversion\n",
    "    for ticker in convert_tickers:\n",
    "        if ticker in log_returns.columns:\n",
    "            log_returns[ticker] = np.log(log_returns[ticker] / log_returns[ticker].shift(1))\n",
    "    \n",
    "    # Drop the first row which will have NaN values due to the shift\n",
    "    log_returns = log_returns.dropna()\n",
    "    \n",
    "    return log_returns\n",
    "\n",
    "log_return_eco = convert_to_log_returns(eco_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_return_eco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_lstm(df, economic_indicators, sequence_length=20):\n",
    "    \n",
    "    merged_df = df.join(economic_indicators, how='inner')\n",
    "    \n",
    "    # Calculate additional features\n",
    "    merged_df['volatility_ftse'] = df['FTSE100'].pct_change().rolling(window=20).std()\n",
    "    merged_df['volatility_pf'] = df['total_rep_pf'].pct_change().rolling(window=20).std()\n",
    "    merged_df['spread_ma5'] = df['spread'].rolling(window=5).mean()\n",
    "    merged_df['spread_ma20'] = df['spread'].rolling(window=20).mean()\n",
    "    \n",
    "    merged_df['rsi_spread'] = calculate_rsi(df['spread'], window=14)\n",
    "    merged_df = merged_df.dropna()\n",
    "    \n",
    "    \n",
    "    features = ['SP500', 'VIX', 'TNX', 'FVX', \n",
    "                'GF', 'COF', 'GBPUSD', 'FTSE100', 'total_rep_pf',\n",
    "                'volatility_ftse', 'volatility_pf', 'spread_ma5', 'spread_ma20', 'rsi_spread']\n",
    "    \n",
    "    target = ['spread']\n",
    "\n",
    "    train_size = int(len(merged_df) * 0.8)\n",
    "    train_data = merged_df.iloc[:train_size]\n",
    "    test_data = merged_df.iloc[train_size:]\n",
    "\n",
    "    # feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    feature_scaler = StandardScaler()\n",
    "    target_scaler = StandardScaler()\n",
    "\n",
    "    feature_scaler.fit(train_data[features])\n",
    "    target_scaler.fit(train_data[target])\n",
    "    \n",
    "    train_feature_scaled = feature_scaler.transform(train_data[features])\n",
    "    train_target_scaled = target_scaler.transform(train_data[target])\n",
    "    \n",
    "    test_feature_scaled = feature_scaler.transform(test_data[features])\n",
    "    test_target_scaled = target_scaler.transform(test_data[target])\n",
    "    \n",
    "    X_train, y_train = create_sequences(train_feature_scaled, train_target_scaled, sequence_length)\n",
    "    X_test, y_test = create_sequences(test_feature_scaled, test_target_scaled, sequence_length)\n",
    "    \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, feature_scaler, target_scaler, train_data,test_data,merged_df,features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RSI\n",
    "def calculate_rsi(series, window=14):\n",
    "    \"\"\"Calculate RSI for a price series\"\"\"\n",
    "    delta = series.diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    \n",
    "    avg_gain = gain.rolling(window=window).mean()\n",
    "    avg_loss = loss.rolling(window=window).mean()\n",
    "    \n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(len(X) - seq_length):\n",
    "        X_seq.append(X[i:i+seq_length])\n",
    "        y_seq.append(y[i+seq_length])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    n_features = X_train.shape[2]\n",
    "    \n",
    "    lr=3.28920104987804e-05\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], n_features)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(30, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "economic_indicators=log_return_eco\n",
    "    \n",
    "X_train, X_test, y_train, y_test, feature_scaler,target_scaler,train_data,test_data,merged_df,features= prepare_data_for_lstm(\n",
    "   spread_df , economic_indicators, 20\n",
    ")\n",
    "\n",
    "model, history = build_lstm_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length=20\n",
    "train_pred = model.predict(X_train)\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "train_pred = target_scaler.inverse_transform(train_pred)\n",
    "test_pred = target_scaler.inverse_transform(test_pred)\n",
    "\n",
    "train_indices = train_data.index[sequence_length:]\n",
    "test_indices = test_data.index[sequence_length:]\n",
    "\n",
    "train_predictions_df = pd.DataFrame(\n",
    "    data=train_pred, \n",
    "    columns=['predicted_spread'],\n",
    "    index=train_indices[:len(train_pred)]  # Ensure matching lengths\n",
    ")\n",
    "train_predictions_df['actual_spread'] = train_data.loc[train_indices[:len(train_pred)], 'spread'].values\n",
    "train_predictions_df['dataset'] = 'train'\n",
    "\n",
    "test_predictions_df = pd.DataFrame(\n",
    "    data=test_pred, \n",
    "    columns=['predicted_spread'],\n",
    "    index=test_indices[:len(test_pred)]  # Ensure matching lengths\n",
    ")\n",
    "test_predictions_df['actual_spread'] = test_data.loc[test_indices[:len(test_pred)], 'spread'].values\n",
    "test_predictions_df['dataset'] = 'test'\n",
    "\n",
    "# Combine predictions for full view\n",
    "all_predictions_df = pd.concat([train_predictions_df, test_predictions_df])\n",
    "all_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_window=20\n",
    "test_predictions = all_predictions_df[all_predictions_df['dataset'] == 'test'].copy()\n",
    "\n",
    "actual_spread = test_predictions['actual_spread']\n",
    "predicted_spread = test_predictions['predicted_spread']\n",
    "\n",
    "error = actual_spread - predicted_spread\n",
    "\n",
    "rolling_mean = error.rolling(window=z_score_window).mean()\n",
    "rolling_std = error.rolling(window=z_score_window).std()\n",
    "z_score = (error - rolling_mean) / rolling_std\n",
    "\n",
    "signals = pd.DataFrame(index=test_predictions.index)\n",
    "signals['actual_spread'] = actual_spread\n",
    "signals['predicted_spread'] = predicted_spread\n",
    "signals['error'] = error\n",
    "signals['z_score'] = z_score\n",
    "signals['signal'] = 0 \n",
    "\n",
    "signals.loc[z_score <= -1.5, 'signal'] = 1    # Buy when actual is lower than predicted\n",
    "signals.loc[z_score >= 1.5, 'signal'] = -1    # Sell when actual is higher than predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_df=signals\n",
    "signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window=5\n",
    "eval_df = signals.copy()\n",
    "eval_df['signal'] = eval_df['signal'].fillna(0)\n",
    "eval_df['forward_spread_change'] = eval_df['actual_spread'].shift(-window) - eval_df['actual_spread']\n",
    "\n",
    "buy_success = ((eval_df['signal'] == 1) & (eval_df['forward_spread_change'] > 0))\n",
    "sell_success = ((eval_df['signal'] == -1) & (eval_df['forward_spread_change'] < 0))\n",
    "hold_success = ((eval_df['signal'] == 0) & \n",
    "               (abs(eval_df['forward_spread_change']) < eval_df['forward_spread_change'].std() * 0.5))\n",
    "\n",
    "buy_success_rate = buy_success.sum() / (eval_df['signal'] == 1).sum() if (eval_df['signal'] == 1).sum() > 0 else 0\n",
    "sell_success_rate = sell_success.sum() / (eval_df['signal'] == -1).sum() if (eval_df['signal'] == -1).sum() > 0 else 0\n",
    "hold_success_rate = hold_success.sum() / (eval_df['signal'] == 0).sum() if (eval_df['signal'] == 0).sum() > 0 else 0\n",
    "\n",
    "overall_success = buy_success.sum() + sell_success.sum() + hold_success.sum()\n",
    "total_signals = len(eval_df.dropna())\n",
    "overall_success_rate = overall_success / total_signals if total_signals > 0 else 0\n",
    "\n",
    "evaluation_metrics= {\n",
    "    'buy_success_rate': buy_success_rate,\n",
    "    'sell_success_rate': sell_success_rate,\n",
    "    'hold_success_rate': hold_success_rate,\n",
    "    'overall_success_rate': overall_success_rate,\n",
    "    'buy_count': (eval_df['signal'] == 1).sum(),\n",
    "    'sell_count': (eval_df['signal'] == -1).sum(),\n",
    "    'hold_count': (eval_df['signal'] == 0).sum()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStrategy Evaluation:\")\n",
    "print(f\"Buy Signal Success Rate: {evaluation_metrics['buy_success_rate']:.2%}\")\n",
    "print(f\"Sell Signal Success Rate: {evaluation_metrics['sell_success_rate']:.2%}\")\n",
    "print(f\"Hold Signal Success Rate: {evaluation_metrics['hold_success_rate']:.2%}\")\n",
    "print(f\"Overall Success Rate: {evaluation_metrics['overall_success_rate']:.2%}\")\n",
    "print(f\"Buy Signals: {evaluation_metrics['buy_count']}\")\n",
    "print(f\"Sell Signals: {evaluation_metrics['sell_count']}\")\n",
    "print(f\"Hold Signals: {evaluation_metrics['hold_count']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 20))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "\n",
    "# Plot training data predictions\n",
    "train_data = all_predictions_df[all_predictions_df['dataset'] == 'train']\n",
    "plt.plot(train_data.index, train_data['actual_spread'], \n",
    "         color='blue', label='Actual (Train)')\n",
    "plt.plot(train_data.index, train_data['predicted_spread'], \n",
    "         color='green', linestyle='--', label='Predicted (Train)')\n",
    "\n",
    "# Plot test data predictions\n",
    "test_data = all_predictions_df[all_predictions_df['dataset'] == 'test']\n",
    "plt.plot(test_data.index, test_data['actual_spread'], \n",
    "         color='red', label='Actual (Test)')\n",
    "plt.plot(test_data.index, test_data['predicted_spread'], \n",
    "         color='orange', linestyle='--', label='Predicted (Test)')\n",
    "\n",
    "plt.title('Actual vs Predicted Spread')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Spread')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 3: Trading signals\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(signals_df['actual_spread'], label='Actual Spread', color='blue')\n",
    "\n",
    "# Plot buy signals\n",
    "buy_signals = signals_df[signals_df['signal'] == 1].copy()\n",
    "if not buy_signals.empty:\n",
    "    plt.scatter(buy_signals.index, buy_signals['actual_spread'], \n",
    "            color='green', marker='^', s=100, label='Buy')\n",
    "\n",
    "# Plot sell signals\n",
    "sell_signals = signals_df[signals_df['signal'] == -1].copy()\n",
    "if not sell_signals.empty:\n",
    "    plt.scatter(sell_signals.index, sell_signals['actual_spread'], \n",
    "            color='red', marker='v', s=100, label='Sell')\n",
    "\n",
    "plt.title('Trading Signals')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Spread')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature importance\n",
    "baseline_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "importance_scores = []\n",
    "\n",
    "for i in range(X_test.shape[2]):\n",
    "    X_test_permuted = X_test.copy()\n",
    "    X_test_permuted[:, :, i] = np.random.permutation(X_test_permuted[:, :, i])\n",
    "    \n",
    "    permuted_loss = model.evaluate(X_test_permuted, y_test, verbose=0)\n",
    "    importance = permuted_loss - baseline_loss\n",
    "    importance_scores.append(importance)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': importance_scores\n",
    "})\n",
    "\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 14))\n",
    "gs = fig.add_gridspec(3, 2)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[2, 0])\n",
    "ax1.hist(signals_df['z_score'].dropna(), bins=30, color='skyblue', edgecolor='black')\n",
    "ax1.axvline(x=1.5, color='red', linestyle='--', label='Sell Threshold')\n",
    "ax1.axvline(x=-1.5, color='green', linestyle='--', label='Buy Threshold')\n",
    "ax1.set_title('Z-Score Distribution')\n",
    "ax1.legend()\n",
    "ax1.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicator = 'VIX' \n",
    "# plt.scatter(merged_df[indicator], merged_df['spread'], alpha=0.5)\n",
    "# plt.title(f'Spread vs {indicator}')\n",
    "# plt.xlabel(indicator)\n",
    "# plt.ylabel('Spread')\n",
    "# plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "\n",
    "# masker = shap.maskers.Independent(X_test)  \n",
    "# explainer = shap.Explainer(model, masker)  \n",
    "# shap_values = explainer(X_test,max_evals='auto')\n",
    "\n",
    "# shap.summary_plot(shap_values, features=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras_tuner\n",
    "\n",
    "def get_tuned_lstm(hp):  # Accept hp as an argument\n",
    "    n_features = X_train.shape[2]\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Use hyperparameters for tuning the LSTM layers\n",
    "    model.add(LSTM(\n",
    "        hp.Int('units_1', min_value=10, max_value=150, step=20),\n",
    "        activation='relu', \n",
    "        return_sequences=True, \n",
    "        input_shape=(X_train.shape[1], n_features)\n",
    "    ))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "        hp.Int('units_2', min_value=10, max_value=100, step=20),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Use hyperparameter for learning rate tuning\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('lr', min_value=1e-5, max_value=1e-2, sampling='log')),\n",
    "        loss='mse'\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import RandomSearch\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    get_tuned_lstm, \n",
    "    objective='val_loss',  \n",
    "    max_trials=10,         \n",
    "    executions_per_trial=1, \n",
    "    directory='my_dir',    \n",
    "    project_name='lstm_tuning' \n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=50, validation_data=(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = tuner.oracle.get_best_trials()[0]\n",
    "best_hyperparameters = best_trial.hyperparameters.values\n",
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model.save(\"hackathon_attendance_model.h5\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
